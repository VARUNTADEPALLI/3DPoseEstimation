{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOXswh6ocan5kTHSdmdAtBf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose\n","\n","  [arxiv](https://arxiv.org/abs/1811.12004) | [Github](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch)\n","\n","  Osokin, Daniil\n"],"metadata":{"id":"Okud3fBfqTxu"}},{"cell_type":"markdown","source":["Miniconda Installation"],"metadata":{"id":"ASt1V_BFqhxi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HD99KiWjoq7r"},"outputs":[],"source":["!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n","!chmod +x Miniconda3-latest-Linux-x86_64.sh\n","!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local"]},{"cell_type":"markdown","source":["Create conda environment"],"metadata":{"id":"2OprOu3Fqlg6"}},{"cell_type":"code","source":["    # Update conda\n","!conda update -n base -c defaults conda -y\n","\n","\n","import sys\n","_ = (sys.path\n","          .append(\"/usr/local/lib/python3.9/site-packages\"))\n","\n","  # Create a new environment\n","!conda create --name gtrs python=3.9 -y"],"metadata":{"id":"xGaSlah8owSX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Activate conda environment"],"metadata":{"id":"d7R3Qd5jqoha"}},{"cell_type":"code","source":["!conda init bash\n","!source activate gtrs"],"metadata":{"id":"B1Byuk7SowO_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!conda update -n base -c defaults conda -y"],"metadata":{"id":"MZ04MD2wowMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install torch torchvision torchaudio"],"metadata":{"id":"ZjUhK0vTqrFm"}},{"cell_type":"code","source":["!pip uninstall torch\n","!pip cache purge\n","!pip install torch torchvision --pre -f https://download.pytorch.org/whl/nightly/cu121/torch_nightly.html\n","!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"],"metadata":{"id":"CPb6DQGvowKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Quick demo"],"metadata":{"id":"ntTUhfxdrB4M"}},{"cell_type":"markdown","source":["Download the project from https://drive.google.com/drive/folders/16913ljV7Tt6FmRR7uGaCLyIAe649mdeS?usp=sharing and mount it to drive to run the inference."],"metadata":{"id":"WxYNL3DcrGsj"}},{"cell_type":"markdown","source":["Install required packages"],"metadata":{"id":"keQIsl-NqyTj"}},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/TransPose/requirements.txt"],"metadata":{"id":"oFqUlMxzowHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["certain changes have to be made before the demo is run.\n","\n","1. change the location of where the result.npy shoulbe saved at line number 142 at demo.py\n","\n","2. change the location of where the output image on which the 2pose is drawn at line number 151 at demo.py"],"metadata":{"id":"8VfO2Qnlrfw3"}},{"cell_type":"markdown","source":["Run a quick demo on the downloaded checkpoint and an input image"],"metadata":{"id":"3bJ22-PBq3Zr"}},{"cell_type":"code","source":["!python /content/drive/MyDrive/lightweight/demo.py --checkpoint-path /content/drive/MyDrive/checkpoint_iter_370000.pth --images /content/drive/MyDrive/lightweight/input.jpeg"],"metadata":{"id":"pI_sqJxfowEt"},"execution_count":null,"outputs":[]}]}